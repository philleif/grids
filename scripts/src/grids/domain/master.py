"""Domain master agent -- owns the deep skill, specifies work, validates output."""

import json

from langchain_core.messages import HumanMessage, SystemMessage

from grids.domain.config import DomainConfig, SubAgentConfig
from grids.domain.sub_agent import SubAgent, SubAgentScore
from grids.knowledge.store import query_store
from grids.orchestration.agents import get_llm


class WorkSpec:
    """Structured work specification generated by the master agent."""

    def __init__(self, raw: dict):
        self.title: str = raw.get("title", "")
        self.description: str = raw.get("description", "")
        self.components: list[dict] = raw.get("components", [])
        self.acceptance_criteria: list[str] = raw.get("acceptance_criteria", [])
        self.estimated_size: float = raw.get("estimated_size", 1.0)
        self.raw = raw

    def to_dict(self) -> dict:
        return self.raw


class ValidationResult:
    """Aggregated validation result from sub-agents + master veto."""

    def __init__(
        self,
        approved: bool,
        weighted_score: float,
        master_score: float,
        sub_scores: list[SubAgentScore],
        feedback: str,
        iteration: int,
    ):
        self.approved = approved
        self.weighted_score = weighted_score
        self.master_score = master_score
        self.sub_scores = sub_scores
        self.feedback = feedback
        self.iteration = iteration

    def to_dict(self) -> dict:
        return {
            "approved": self.approved,
            "weighted_score": round(self.weighted_score, 3),
            "master_score": round(self.master_score, 3),
            "sub_scores": [s.to_dict() for s in self.sub_scores],
            "feedback": self.feedback,
            "iteration": self.iteration,
        }


def _build_master_prompt(config: DomainConfig, knowledge_context: str) -> str:
    """Auto-generate the master's system prompt from config and knowledge."""
    base = (
        f"You are the master domain agent for: {config.domain.name}\n"
        f"Domain: {config.domain.description}\n\n"
        "You are the senior expert. You synthesize across all sub-specialties "
        "and have final authority on whether work product meets domain standards.\n\n"
        "Your sub-agents and their specialties:\n"
    )
    for sa in config.sub_agents:
        base += f"- {sa.name}: {sa.aspect} (strictness: {sa.strictness})\n"

    if knowledge_context:
        base += (
            "\n\nYour expertise is grounded in these reference materials:\n"
            f"{knowledge_context}\n"
        )

    return base


def _retrieve_master_context(config: DomainConfig, query: str, n: int = 5) -> str:
    """Retrieve context from all domain knowledge collections."""
    parts = []
    for coll in config.master.knowledge_collections:
        try:
            hits = query_store(query, coll, n_results=n)
            for hit in hits:
                section = hit.get("metadata", {}).get("section", "")
                source = hit.get("metadata", {}).get("source", coll)
                excerpt = hit["text"][:500]
                parts.append(f"[{source} / {section}]\n{excerpt}")
        except Exception:
            continue
    return "\n---\n".join(parts)


class DomainMaster:
    """Master agent for a domain -- specifies work, aggregates validation, has veto."""

    def __init__(self, config: DomainConfig):
        self.config = config
        self.sub_agents = [SubAgent(sa, config) for sa in config.sub_agents]

    def specify(self, request: str) -> WorkSpec:
        """Turn a user request into a structured work specification."""
        llm = get_llm(temperature=0.4)

        context = _retrieve_master_context(self.config, request, n=5)
        system = _build_master_prompt(self.config, context)

        messages = [
            SystemMessage(content=system),
            HumanMessage(content=(
                f"Generate a structured work specification for this request:\n\n{request}\n\n"
                "Output JSON with these fields:\n"
                "- title: concise title\n"
                "- description: detailed description of what to build\n"
                "- components: list of {name, description, type} for each piece\n"
                "- acceptance_criteria: list of specific, testable criteria\n"
                "- estimated_size: effort estimate (1.0 = average)\n\n"
                "Each acceptance criterion should map to one of your sub-agents' specialties. "
                "Be specific and actionable."
            )),
        ]

        response = llm.invoke(messages)
        text = response.content.strip()

        # Parse JSON from response
        try:
            if "```" in text:
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
                text = text.strip()
            parsed = json.loads(text)
        except (json.JSONDecodeError, IndexError):
            parsed = {
                "title": request[:80],
                "description": text,
                "components": [],
                "acceptance_criteria": [],
                "estimated_size": 1.0,
            }

        return WorkSpec(parsed)

    def validate(self, work_product: dict, brief: str, iteration: int = 0) -> ValidationResult:
        """Run sub-agent scoring, aggregate, and apply master veto."""
        # Sub-agents score independently
        sub_scores: list[SubAgentScore] = []
        for sa in self.sub_agents:
            score = sa.score(work_product, brief)
            sub_scores.append(score)

        # Weighted average by strictness
        total_weight = sum(sa.config.strictness for sa in self.sub_agents) or 1.0
        weighted_score = sum(
            s.score * sa.config.strictness
            for s, sa in zip(sub_scores, self.sub_agents)
        ) / total_weight

        # Master's own assessment
        master_score = self._master_score(work_product, brief, sub_scores)

        # Decision
        rules = self.config.rules
        approved = (
            weighted_score >= rules.approval_threshold
            and master_score >= rules.master_veto_threshold
        )

        # If not approved, generate feedback
        feedback = ""
        if not approved:
            feedback = self._generate_feedback(work_product, brief, sub_scores, master_score)

        return ValidationResult(
            approved=approved,
            weighted_score=weighted_score,
            master_score=master_score,
            sub_scores=sub_scores,
            feedback=feedback,
            iteration=iteration,
        )

    def _master_score(self, work_product: dict, brief: str, sub_scores: list[SubAgentScore]) -> float:
        """Master agent's own holistic score of the work product."""
        llm = get_llm(temperature=0.2)

        context = _retrieve_master_context(self.config, brief, n=3)
        system = _build_master_prompt(self.config, context)

        sub_summary = "\n".join(
            f"- {s.agent_name}: {s.score:.2f} ({s.verdict}) -- {s.feedback[:100]}"
            for s in sub_scores
        )

        product_str = json.dumps(work_product, indent=2)[:4000]

        messages = [
            SystemMessage(content=system),
            HumanMessage(content=(
                f"As the master domain expert, score this work product holistically.\n\n"
                f"Brief: {brief}\n\n"
                f"Sub-agent scores:\n{sub_summary}\n\n"
                f"Work product:\n{product_str}\n\n"
                "Output ONLY a JSON object: {\"score\": 0.0-1.0, \"reasoning\": \"...\"}"
            )),
        ]

        response = llm.invoke(messages)
        text = response.content.strip()

        try:
            if "```" in text:
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
                text = text.strip()
            parsed = json.loads(text)
            return float(parsed.get("score", 0.5))
        except (json.JSONDecodeError, IndexError, ValueError):
            return 0.5

    def _generate_feedback(
        self, work_product: dict, brief: str,
        sub_scores: list[SubAgentScore], master_score: float,
    ) -> str:
        """Generate specific iteration feedback for execution agents."""
        llm = get_llm(temperature=0.3)

        failing = [s for s in sub_scores if s.verdict != "pass"]
        fail_summary = "\n".join(
            f"- {s.agent_name} ({s.score:.2f}): {s.feedback}"
            for s in failing
        )

        context = _retrieve_master_context(self.config, brief, n=3)
        system = _build_master_prompt(self.config, context)

        messages = [
            SystemMessage(content=system),
            HumanMessage(content=(
                f"The work product did not pass validation.\n\n"
                f"Brief: {brief}\n"
                f"Master score: {master_score:.2f}\n\n"
                f"Failing sub-agent assessments:\n{fail_summary}\n\n"
                f"Generate specific, actionable revision instructions. "
                f"Be concrete about what to change and why, referencing domain principles."
            )),
        ]

        response = llm.invoke(messages)
        return response.content.strip()
